# ISATSleepNet

## Abstract 
Automatic sleep staging is essential for sleep quality assessment, yet current approaches face several challenges: class imbalance in datasets, inadequate modeling of multimodal physiological properties, and insufficient capture of sleep dynamics across timescales. To address these challenges, we propose ISATSleepNet, a  neuro-inspired sleep staging model based on interactive temporal attention. To alleviate sample imbalance, we propose the Physio-GAN framework, which introduces temporal and spectral consistency constraints during the generation process to ensure that synthetic samples remain physiologically consistent with real ones. Efficient multimodal feature extraction is achieved via a neuro-inspired multi-scale EEG extractor, which simulates the brainâ€™s hierarchical processing of sleep rhythms across different temporal scales, capturing transient and rhythmic neural dynamics through multi-scale convolution and channel-wise rescaling, and an ocular dynamics module, discriminating slow and rapid eye movements using dual fast-slow pathways with large-kernel residuals and temporal pyramid pooling. To jointly capture local and global temporal dependencies, we further propose a cross-modal interactive temporal attention sequence module, which integrates modality interaction and temporal dependency in a unified manner, analogous to the exchange and integration of information across multisensory pathways in the brain. Experimental results on three public benchmark datasets demonstrate that the proposed method consistently outperforms existing state-of-the-art approaches, achieving accuracies of 89.1% on Sleep-EDF-20, 87.1% on Sleep-EDF-78, and 89.7% on SHHS, which validates the effectiveness and robustness of the proposed architecture.
